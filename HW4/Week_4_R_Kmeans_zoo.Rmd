---
title: "R_KMeans_Clustering_Zoo"
author: "Yang"
date: "3/1/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document serves as an introduction to the k-means clustering method.

## Load R Packages

Install required R packages:

```{r}
#install.packages('RWeka')
#install.packages('tidyverse')  # data manipulation
#install.packages('cluster')    # clustering algorithms
#install.packages('factoextra') # clustering algorithms & visualization
#install.packages('gridExtra')
```

```{r}
library(RWeka)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(gridExtra)  # subfigure layout package
```

## Data Preparation

Setup the working environment and load the zoo.csv file

```{r}
setwd("C:/Users/Yang/R_Workspace/Week_4")
zoo <- read.csv("zoo.csv")
str(zoo)

typeof(zoo)
```

To remove any missing value that might be present in the data, type this:

```{r}
zoo <- na.omit(zoo)
```

Remove the label information

```{r}
zoo_unlabeled <- zoo[,c(2:17)]
head(zoo_unlabeled)
```

As we don’t want the clustering algorithm to depend to an arbitrary variable unit, we start by scaling/standardizing the data using the R function scale:

optional for zoo data.
```{r}
zoo_unlabeled <- scale(zoo_unlabeled)
head(zoo_unlabeled)

typeof(zoo_unlabeled)
```

```{r}
distance <- get_dist(zoo_unlabeled)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

## RWeka SimpleKMeans Clustering

```{r}
model_rweka <- SimpleKMeans (zoo_unlabeled, control = Weka_control(N = 7, I=500, S=100))
model_rweka
```

## KMeans Clustering in R

Since it is difficult to interpret and visualize the clustering results with RWeka, we will introduce some built-in R functions, as well as visualization packages.

```{r}
model_r = kmeans(zoo_unlabeled, centers = 7, nstart = 25)
model_r
```

print the centroids

```{r}
model_r$centers
```

get cluster assignment

```{r}
cluster_assignment <- data.frame(zoo,model_r$cluster)
#View(cluster_assignment)
```

visualize animal types and clusters by specific features, red = milk yes, black = milk no.
```{r}
plot(zoo$type ~ jitter(model_r$cluster, 1), pch=21,col=as.factor(zoo$milk))
```

We can also view our results by using fviz_cluster. If there are more than two variables fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

```{r}
fviz_cluster(model_r, data = zoo_unlabeled)
```

## Select the best K

It is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure:

```{r}
k3 <- kmeans(zoo_unlabeled, centers = 3, nstart = 25)
k4 <- kmeans(zoo_unlabeled, centers = 4, nstart = 25)
k5 <- kmeans(zoo_unlabeled, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k3, geom = "point",  data = zoo_unlabeled) + ggtitle("k = 3")
p2 <- fviz_cluster(k4, geom = "point",  data = zoo_unlabeled) + ggtitle("k = 4")
p3 <- fviz_cluster(k5, geom = "point",  data = zoo_unlabeled) + ggtitle("k = 5")
p4 <- fviz_cluster(model_r, geom = "point", data = zoo_unlabeled) + ggtitle("k = 7")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Preferably we would like to use the optimal number of clusters. The following explains the three most popular methods for determining the optimal clusters, which includes:

* Elbow method
* Silhouette method
* Gap statistic

### Elbow method

```{r}
set.seed(123)

# set up a list that records cluster size from 1 to 8 
k.values <- 1:8

# set up an empty list, which will be used to record within-cluster SSE for each k
sse.values <- vector(mode = "list", length = length(k.values))

# compute total within-cluster sum of square for each possible k
for (i in k.values) {
  sse.values[i] = kmeans(zoo_unlabeled, i)$tot.withinss
}

# plot the relationship between k and sse
plot(k.values, sse.values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

```

Fortunately, this process to compute the “Elbow method” has been wrapped up in a single function (fviz_nbclust):

```{r}
set.seed(123)
fviz_nbclust(zoo_unlabeled, kmeans, method = "wss", k.max=8)
```

### Average Silhouette Method
In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.

```{r}
set.seed(123)
# set up a list that records cluster size from 2 to 8, because silhouette does not apply to k=1
k.values <- 1:8

# set up an empty list, which will be used to record average Silhouette
sil.values <- vector(mode = "list", length = length(k.values))

# compute average Silhouette for each possible k
for (i in k.values) {
  if (i == 1) {
    sil.values[i] = 0
  } else {
    km.res = kmeans(zoo_unlabeled, i)
    ss <- silhouette(km.res$cluster, dist(zoo_unlabeled))
    print(i)
    sil.values[i] = mean(ss[,3])
  }
}  

# plot the relationship between k and Silhouette
plot(k.values, sil.values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")

```

Similar to the elbow method, this process to compute the “average silhoutte method” has been wrapped up in a single function

```{r}
set.seed(123)
fviz_nbclust(zoo_unlabeled, kmeans, method = "silhouette")
```

### Gap Statistic Method

The gap statistic has been published by R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001). The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering).

Choose the number of clusters as the smallest k such that
$gap(k) > gap(k+1) - s_{k+1}$ where $s_{k+1}$ is the standard deviation.

```{r}
set.seed(123)
gap_stat <- clusGap(zoo_unlabeled, FUN = kmeans,
                    K.max = 8, B = 50)

set.seed(123)
fviz_gap_stat(gap_stat)
```

### Extracting Results

```{r}
set.seed(123)
final.res <- kmeans(zoo_unlabeled, 6, nstart = 25)

fviz_cluster(final.res, data = zoo_unlabeled)

zoo_unlabeled$cluster <- final.res$cluster

zoo_unlabeled %>%
  group_by(cluster) %>%
  summarise_all("mean")

```