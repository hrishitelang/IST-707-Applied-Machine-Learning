---
output:
  pdf_document: default
  html_document: default
---
# HW4 - Clustering
Using K-Means and HAC, you are going to try solving this mystery using clustering algorithms. Document your analysis process and draw your conclusion on who wrote the disputed essays. Provide evidence for each method to demonstrate what patterns had been learned to predict the disputed papers, for example, visualize the clustering results and show where the disputed papers are located in relation to Hamilton and Madison's papers. By the way, where are the papers with joint authorship located? For k-Means, analyze the centroids to explain which attributes are most useful for clustering. Hint: the centroid values on these dimensions should be far apart from each other to be able to distinguish the clusters. 

```{r}
#Name: Hrishikesh Telang
```

## I am loading the required packages
```{r,eval=TRUE, results='hide', message=FALSE, warning=FALSE}
library(factoextra)
library(stringr)
library(rpart)
library(caret)
library(gridExtra)
library(tidyr)
```

## Now I am loading the dataset
```{r}
df <- read.csv("HW4-data-fedPapers85.csv")
```

## I am trying to check the structure of the dataframe
```{r}
str(df)
```

## I am trying to check the structure of the dataframe
```{r}
View(df)
```

# Data Manipulation to label the data points:
## I am creating a new column with a short form of the author name and storing it as modified_author:
```{r}
df$modified_author <- ifelse(df$author == 'dispt', 'D', ifelse(df$author == 'Hamilton', 'H', ifelse(df$author == 'HM', 'HM', ifelse(df$author == 'Jay', "J", ifelse(df$author == 'Madison', 'M', NA)))))
df$modified_author
```

## I am splitting the file name and number into Name and Num:
```{r}
df <- extract(df, filename, into = c("Name", "Num"), "([^(]+)\\s*[^0-9]+([0-9].).")
```

## I am creating a new column combining the author name along with the file number:
```{r}
df$file <- paste(df$modified_author, "-", df$Num)
```

## I convert the column to index:
```{r}
rownames(df)<-df$file
head(df, 5)
```
## I remove all the useless columns
```{r}
df <- df[-c(1,2,3)]
df <- df[c(-(ncol(df)))]
head(df, 5)
```
# Dropping the rows of files authored by Jay and Hamilton+Madison:
## As we are only concerned about the authorship of the disputed articles, of Hamilton and of Madison, we are not concerned about those 3 articles written by Hamilton and Madison and 5 written by Jay. Thus, we can go ahead and remove 'Jay' and 'HM' from the dataframe and store it in the dataframe 'subset'.

```{r}
subset <- subset(df, df$modified_author!="J" & df$modified_author!="HM")
head(subset, 5)
```
## Dropping unused levels:
```{r}
subset <- droplevels(subset)
```

## I am just checking the first five rows of subset:
```{r}
head(subset, 5)
```

## I create a copy of subset2:
```{r}
subset2 <- data.frame(subset)
head(subset2, 5)
```

## K-means - Default  
Clustering is an unsupervised learning technique. It is the task of grouping together a set of objects in a way that objects in the same cluster are more similar to each other than to objects in other clusters. Similarity is an amount that reflects the strength of relationship between two data objects. Clustering is mainly used for exploratory data mining. It is used in many fields such as machine learning, pattern recognition, image analysis, information retrieval, bio-informatics, data compression, and computer graphics.

## I use the elbow method directly here. The elbow method helps us check the optimality of the clusters required for analysis:
```{r}
set.seed(6)
wcss = vector()
for (i in 1:10) wcss[i] = sum(kmeans(subset2[1:(length(subset2)-1)], i)$withinss)
plot(1:10,
     wcss,
     type = 'b',
     main = paste('The Elbow Method'),
     xlab = 'Number of clusters',
     ylab = 'WCSS')
```
From the above graph, it is safe to say that 5 or 6 are the optimal number of clusters for this dataset. For this example, let us consider 5 clusters

# We train the K Means algorithm by removing the last column and taking 5 clusters:
```{r}
set.seed(29)
kmeans = kmeans(x = subset2[1:(length(subset2)-1)], centers = 5)
y_kmeans = kmeans$cluster
```

# We check how many disputed articles were linked with which author:
```{r}
t <- t(table(subset[,length(subset)], y_kmeans))
t
```

We can clearly see that the disputed articles are authored by Madison as there is a strong link of similarity there.

# Visualising the clusters
```{r}
library(cluster)
clusplot(subset,
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Clusters of customers'),
         xlab = 'Dim 1',
         ylab = 'Dim 2')
```
# Hierarchical Clustering
Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.

## Using the dendrogram to find the optimal number of clusters
```{r, fig.width=12, fig.height=7}
dendrogram = hclust(d = dist(subset2[1:(length(subset2)-1)], method = 'euclidean'), method = 'ward.D')
plot(dendrogram,
     main = paste('Dendrogram using HAC Algorithm'),
     xlab = 'Customers',
     ylab = 'Euclidean distances')
```

If we observe the dendogram carefully, we see that the disputed articles in each tail is associated with Hamilton

## Fitting Hierarchical Clustering to the dataset
```{r}
hc = hclust(d = dist(subset, method = 'euclidean'), method = 'ward.D')
y_hc = cutree(hc, 4)
```

## Visualising the clusters
```{r}
library(cluster)
clusplot(subset,
         y_hc,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels= 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste('Clusters of customers'),
         xlab = 'Annual Income',
         ylab = 'Spending Score')
```

Conclusion: The disputed articles were authored by Madison as confirmed by K-Means and HAC.